{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 4 - Logistic Regression and Softmax.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "O7jH80Rz2dyF",
        "SR7Mn4DuKk4m",
        "38ZZxbEZKqy0",
        "eDsknqV62_89",
        "4M6GjGtL8VW-",
        "JrCzemMZUPnz",
        "oLsE854EUkeB",
        "ZlsXXwQdBBdN",
        "Sq1Exa6TLFui",
        "fl6vO4IFwTM3"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oh0MSadCxFuF"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7jH80Rz2dyF",
        "colab_type": "text"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SR7Mn4DuKk4m",
        "colab_type": "text"
      },
      "source": [
        "### Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vgHy1qkcXmVT"
      },
      "source": [
        "*  Logistic regression is a model used to relate **categorical** output to one or several **quantitative** inputs;\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "* For example one can use logistic regression to determine from a given weight and height whether the person is a child or an adult;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Below is the illustration of two samples of children and adults\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=12gtdjxV3B-_m44yluVAilua0_TFX_YYJ)\n",
        "\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where the green line is the model that separates between the two categories whose expression is \n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\mbox{height} = - \\mbox{weight} + 180 \\iff  \\mbox{weight} + \\mbox{height}  - 180 = 0.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The category is obtained by the following function of $\\mbox{weight}$ and $\\mbox{height}$  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\mbox{weight},\\mbox{height}) = \\mbox{weight} + \\mbox{height}  - 180,\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where :\n",
        "  \n",
        "  * $f(\\mbox{weight},\\mbox{height}) > 0$ means the point defined with the coordinates $(\\mbox{weight},\\mbox{height})$  lies **above** the green line and  belongs to the category of **adults**;\n",
        "  \n",
        "  * $f(\\mbox{weight},\\mbox{height}) < 0$ the point is **below** the green line and the person is classified amongst **children**;\n",
        "  \n",
        "  * $f(\\mbox{weight},\\mbox{height}) = 0$ the person lies **on** the green line and is considered to be between the two categories .\n",
        "  \n",
        "\\\\\n",
        "\n",
        "* The quantity $f(\\mbox{weight},\\mbox{height})$ is refered as the ***score***  and it is **translated** into **probability**  through  ***Logistic Function*** as follows\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "P\\left(y = Adult | \\mbox{weight},\\mbox{height}\\right) = \\frac{1}{1 + e^{-f(\\mbox{weight},\\mbox{height})}}.\n",
        "\\end{equation}\n",
        "  \n",
        "\\\\\n",
        "\n",
        "* Indeed we can see from the plot below of the logistic function that any $z \\in {\\rm I\\!R}$ is turned into an output $\\in [0,1]$.\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1383aDQg_sGYeYUhk2pFwNNX5TRIzJlwh)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38ZZxbEZKqy0",
        "colab_type": "text"
      },
      "source": [
        "### Sign of $f(\\mbox{weight},\\mbox{height})$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myeLNgHLKrI4",
        "colab_type": "text"
      },
      "source": [
        "* To fully understand how the sign of $f(\\mbox{weight},\\mbox{height})$ matches the different categories keep in mind that\n",
        "\n",
        "\\\\\n",
        "\n",
        "* **points** in the figure \n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=12gtdjxV3B-_m44yluVAilua0_TFX_YYJ)\n",
        "\n",
        "* are mere **representations** of **vectors** and so the real graphical representation looks like\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1zK5dFGTpEB2IkkkybWCKuomulVyCSJp5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8vXVZQLKrm2",
        "colab_type": "text"
      },
      "source": [
        "* To keep things simple, consider centered data with a separator with intercept equals to zero whose expression is \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\mbox{weight},\\mbox{height}) = \\mbox{weight} + \\mbox{height},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* illustrated below\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1bG8WQKNcAP5Ws_XHQJEbk_xRceeO1zkC)\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "* $f(\\mbox{weight},\\mbox{height})$ can be expressed as a dot product between $\\bf{w} = [1,1]$ and $\\bf{x} = [\\mbox{weight},\\mbox{height}]^T$ \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "f(\\mbox{weight},\\mbox{height})   = [1, 1] \n",
        "\\begin{bmatrix}\n",
        "\\mbox{weight}\\\\\n",
        "\\mbox{height}\n",
        "\\end{bmatrix} = \n",
        "\\bf{w}\\bf{x}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1MBJK7RpRo1ZVBIYy1I1OFJVL1pRCBNvl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4bxMHJJKRRE",
        "colab_type": "text"
      },
      "source": [
        "* In the figure above, we can visualize the vector $\\bf{w}$ (in orange and which happens to be perpendicular to the seperator):\n",
        "  * red vectors which form angles $\\theta \\in \\left] \\frac{3\\pi}{2},\\frac{\\pi}{2} \\right[$ with $\\bf{w}$ result into positive dot products and so  $f(\\mbox{weight},\\mbox{height}) > 0$;\n",
        "\n",
        "  * vectors lying on the green line form two possible angles $\\theta \\in \\{ \\frac{\\pi}{2},\\frac{3\\pi}{2} \\}$ both leading to null $\\cos(\\theta)$ and so $f(\\mbox{weight},\\mbox{height}) = 0$;  \n",
        "\n",
        "  * blue vectors form angles $\\theta \\in \\left] \\frac{\\pi}{2},\\frac{3\\pi}{2} \\right[$ with $\\bf{w}$ and result into negative dot products and so  $f(\\mbox{weight},\\mbox{height}) < 0$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDsknqV62_89",
        "colab_type": "text"
      },
      "source": [
        "## Formalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oEc5op5Vsfma"
      },
      "source": [
        "*  Formally speaking  binary logistic regression is the probability that $\\bf{x} = [x_1,\\ldots,x_d]^T$ belongs to class $1$ \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "P(y = 1| \\bf{x}) = \\frac{1}{1 + e^{-(\\bf{w}^T\\bf{x} + b})},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where $\\bf{w}^T \\in  {\\rm I\\!R}^d$ is the vector of weights and $b \\in  {\\rm I\\!R}$ is the bias.\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "* Please note the following\n",
        "\n",
        "  * The model computes for a given observation $\\bf{x}$  the belonging probability to one class **only**;\n",
        "  \n",
        "  * The belonging probability to the other class is deduced by the complementary rule of probability \n",
        "  \n",
        "\\\\\n",
        "  \n",
        "  \\begin{equation}\n",
        "  P(y = 0 | \\bf{x}) = 1 -P(y = 1| \\bf{x}) .\n",
        "  \\end{equation}\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7B89fmzzWGeg"
      },
      "source": [
        "# Softmax Regression\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M6GjGtL8VW-",
        "colab_type": "text"
      },
      "source": [
        "## Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iEpN6Lx_Bi3v"
      },
      "source": [
        "\n",
        "\n",
        "* Softmax regression is the extension  of logistic regression over $C$ classes.\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1zs42I6MSrdWTO-dcuGGU4ijy_qi7II2g)\n",
        "\n",
        "\\\\\n",
        "\n",
        "* To deal with $C$ classes problem we define $C$ binary regressions where each category discriminates its specific class over all the others.\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1GiF3jgiRmllA0NYO0_dIvgttKBuIc07n)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aIBTT4L9BveF"
      },
      "source": [
        "*  So for a given data point $\\bf{x} \\in {\\rm I\\!R^d}$ softmax regression  calculates the belonging probability to every class, the output is a vector of probabilities \n",
        "  \n",
        "  \n",
        "\\\\\n",
        "  \n",
        "  \\begin{equation}\n",
        "  \\begin{bmatrix}\n",
        "  P(y = 1 | \\bf{x})\\\\\n",
        "  \\vdots\\\\\n",
        "  P(y = C | \\bf{x}),\n",
        "  \\end{bmatrix}\n",
        "  \\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* such as  $\\sum_{k = 1}^C  P(y = k | \\bf{x}) = 1$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z4vktFWGsKUm"
      },
      "source": [
        "* The vector of probabilities above is obtained through the vector of scores $\\bf{s}$ \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{s} = \n",
        "\\begin{bmatrix}\n",
        "s_1\\\\\n",
        "\\vdots\\\\\n",
        "s_C\n",
        "\\end{bmatrix}\n",
        "= \\bf{W} \\bf{x} + b = \n",
        "\\begin{bmatrix}\n",
        "w_{11} & \\ldots & w_{1d}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "w_{C1} & \\ldots & w_{Cd}\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_1 \\\\\n",
        "\\vdots\\\\\n",
        "x_d\n",
        "\\end{bmatrix} + \n",
        "\\begin{bmatrix}\n",
        "b_1\\\\\n",
        "\\vdots\\\\\n",
        "b_C\n",
        "\\end{bmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where $\\bf{W} \\in  {\\rm I\\!R^{C \\times d}}$ and $\\bf{b} \\in  {\\rm I\\!R^C}$ are respectively the matrix of weights and the vector of biaises.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The $k^{th}$ row of $\\bf{W}$ with the $k^{th}$ element of $\\bf{b}$ list the parameters of the logistic regression that classifies the elements of the class $k$ against all the others. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GPQiM1E7sG9E"
      },
      "source": [
        "* The vector $\\bf{s}$ is then transformed into vector of probabilities $\\bf{\\hat{p}} \\in  {\\rm I\\!R^C}$ as follows\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{\\hat{p}}=\n",
        "\\begin{bmatrix}\n",
        "\\hat{p}_1\\\\\n",
        "\\vdots\\\\\n",
        "\\hat{p}_C\\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "\\frac{e^{s_1}}{\\sum_{i = 1}^{C} e^{s_i}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{e^{s_C}}{\\sum_{i = 1}^{C} e^{s_i}}\\\\\n",
        "\\end{bmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The function that transforms each score $s_k$ from the vector $\\bf{s}$  into probability is called ***softmax***.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Note however to transform $s_k$ into $\\hat{p}_k$ the **scalar** softmax function requires the other scores $\\{s_i, i \\neq k \\}$ \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\mbox{softmax}(k,\\bf{s}) = \\frac{e^{s_k}}{\\sum_{j = 1}^{C} e^{s_j}}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* **Vector** version of softmax function is defined as\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\mbox{softmax}(\\bf{s}) = \n",
        "\\begin{bmatrix}\n",
        "\\mbox{softmax}(1,\\bf{s})\\\\\n",
        "\\vdots\\\\\n",
        "\\mbox{softmax}(C,\\bf{s})\n",
        "\\end{bmatrix}. \n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Finally the softmax of any $\\bf{x}$ is compactly expressed as \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{\\bf{p}} =  \\mbox{softmax}(\\bf{W} \\bf{x} + \\bf{b}).\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JrCzemMZUPnz"
      },
      "source": [
        "## Formalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "psd-N74lFzQn"
      },
      "source": [
        "* Let $\\bf{X}$ be a dataset of $n$ examples described through $d$ variables and $\\bf{y}$  the corresponding vector of outputs.\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{X} = \n",
        "\\begin{bmatrix}\n",
        "x_{11} & \\ldots & x_{1d}\\\\\n",
        "\\vdots & \\ddots  & \\vdots \\\\\n",
        "x_{n1} &\\ldots & x_{nd}\n",
        "\\end{bmatrix},\n",
        "\\quad \\bf{y} = \n",
        "\\begin{bmatrix}\n",
        "y_1\\\\\n",
        "\\vdots\\\\\n",
        "y_n\n",
        "\\end{bmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Every $i^{th}$ entry of $\\bf{y}$ represents the  category of the $i^{th}$ row in $\\bf{X}$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Let $C$ be the number of categories. The value of every $y_i$ is an integer between  $1$ and $C$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Training softmax regression model from $\\bf{X}$ first require the transformation of  $\\bf{y}$ into matrix of probabilities  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{P} =\n",
        "\\begin{bmatrix}\n",
        "p_{11} & \\ldots & p_{1C}\\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "p_{n1} & \\ldots & p_{nC}\n",
        "\\end{bmatrix},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where each entry $p_{ik}$ describes the probability that the $i^{th}$ example of $\\bf{X}$ belongs to the class $k$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* It is easy to see that for every row in $\\bf{P}$ only one entry will be set to $1$ while all the others will be set to $0$ in which case rows of $\\bf{P}$ are said to be ***one-hot encoded***.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* In some cases matrix $\\bf{P}$ can represent membership to several classes with different probabilities. \n",
        "\n",
        "\\\\\n",
        "\n",
        "* We define $\\bf{\\hat{P}}$ as the matrix of **estimated probabilities** through softmax regression for some values of $\\bf{W}$ and $\\bf{b}$ \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{\\hat{P}} = \n",
        "\\begin{bmatrix}\n",
        "\\hat{p}_{11} & \\ldots & \\hat{p}_{1k}\\\\\n",
        "\\vdots & \\ddots & \\vdots \\\\\n",
        "\\hat{p}_{n1} & \\ldots & \\hat{p}_{nk}\n",
        "\\end{bmatrix}=\n",
        "softmax(\\bf{X}W^T + \\bf{b}^T) \n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Note the dimensionality of $\\bf{X} \\bf{W}^T$ is $n \\times k$ which makes the sum with $\\bf{b}^T$ impossible;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* To avoid duplicating $\\bf{b}^T$ $n$ times we introduce the operation of ***broadcasting*** which allows $\\bf{b}^T$ to distribute on every row of $\\bf{X} \\bf{W}^T$;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Broadcasting is not a mathematical operation but it is widespread in Machine Learning community (it is supported in *numpy* and *tensorflow*).\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Lasty we define $\\bf{\\hat{y}}$ as the vector of estimated classes \n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{\\hat{y}} =\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}_1\\\\\n",
        "\\vdots\\\\\n",
        "\\hat{y}_n\\\\\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "\\mbox{argmax}(\\bf{\\hat{P}}_{1,:})\\\\\n",
        "\\vdots\\\\\n",
        "\\mbox{argmax}(\\bf{\\hat{P}}_{n,:})\\\\\n",
        "\\end{bmatrix},\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where $\\mbox{argmax}(\\bf{x})$ is a function that for a given vector $\\bf{x}$ returns the index of the biggest value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oLsE854EUkeB"
      },
      "source": [
        "## Loss Function (Cross Entropy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsXXwQdBBdN",
        "colab_type": "text"
      },
      "source": [
        "### Defintion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lfe5GHShUrYa"
      },
      "source": [
        "* In classification models the widespread loss function is known as ***cross entropy*** or ***negative log-likelihood***  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\bf{W}, \\bf{b}) = \\frac{1}{n}\\sum_{i = 1}^n \\sum_{k = 1}^C -p_{i,k} \\log(\\hat{p}_{i,k}),\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* where $p_{i,k}$ and $\\hat{p}_{i,k}$ are respectively the **true** and the **estimated** probabilities that $\\bf{X}_{i,:}$ belongs to class $k$;\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdx1fAXhBEV_",
        "colab_type": "text"
      },
      "source": [
        "### Reflection over the Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1qxjQV2Qh-SM"
      },
      "source": [
        "\n",
        "* If  $P_{i,:}$ are one-hot vectors then  $p_{i,y_i} = 1$  and $\\{p_{i,j} = 0, j \\neq y_i\\}$ thus the sum $\\sum_{j = 1}^k - p_{i,j} \\log(\\hat{p}_{i,j}) = - \\log(\\hat{p}_{i,y_i})$.\n",
        "  * If the predicted probability $\\hat{p}_{i,y_i}$ tends to $1$ its $\\log$  tends to $0$ (see red plot below);\n",
        "\n",
        "  * If however the predicted probability $\\hat{p}_{i,y_i}$ tends to $0$ then its $\\log$  will tend towards $- \\infty$ (see red plot below);\n",
        "\n",
        "  * from this we get that the less precise we predict $\\hat{p}_{i,y_i}$ the bigger will be $- \\hat{p}_{i,j}$ and vice versa (see blue plot below);\n",
        "\n",
        "  * The operation  $ \\frac{1}{n}\\sum_{i = 1}^n$ produces an average of the negative log-likelihoods.\n",
        "\n",
        "\\\\\n",
        "\n",
        "*  In case  $P_{i,:}$ are not one-hot vectors the mechanisms described above still apply.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tY-NU16ab43r"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=19dQnBxZsFehf-TH8ao_cRQ3ksJPlMuea)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aRq2RB8KaZcI"
      },
      "source": [
        "## Estimation of Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sq1Exa6TLFui"
      },
      "source": [
        "### Gradient of Negative Log Likelihood"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HQJgdM8kb9JA"
      },
      "source": [
        "* Minimization of $J$ is achieved through iterative computations of $\\nabla_{\\theta} J$ where $\\theta = [\\bf{W}, \\bf{b}]$;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* Cross-entropy w.r.t weights and biaises is expressed as  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "J(\\bf{W}, \\bf{b}) = \\frac{1}{n}\\sum_{i = 1}^n \\sum_{j = 1}^k -p_{i,j} \\log(\\hat{p}_{i,j}) = \\frac{-1}{n}\\sum_{i = 1}^n \\sum_{j = 1}^k p_{i,j} \\log \\left( \\frac{e^{\\bf{W}_{j,:}\\bf{X}_{:,i}^T + b_j } }{\\sum_{l = 1}^k e^{\\bf{W}_{l,:}\\bf{X}_{:,i}^T + b_l} }  \\right).\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The above equation shows that observations $\\bf{X}_{:,i}$, probabilities $p_{i,j}$ and parameters $\\bf{W}_{i,j}, b_i$ are all  intertwined;\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "* To see clearer we visualize computational graph of $J$ defined on a $2 \\times 2$ dataset $\\bf{X}$ with $2$ categories;\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vU7UhPRACpUsgvcVHLqeO7UbB_b7TpoB)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ARkomimvtgFZ"
      },
      "source": [
        "\n",
        " * Computational graph of $J$ is organized into ***layers*** where:\n",
        "    * Bottom layer contains parameters of the model (weights and bias);\n",
        "\n",
        "    * Parameters are ***flattened*** in the sens we no longer care about  **shapes** of $\\bf{W}$ and $\\bf{b}$;\n",
        "\n",
        "    * Second layer calculates scores for every observation and every class;\n",
        "\n",
        "    * Note that arrows between layers are present only when an explicit relationship exists between the variables;\n",
        "\n",
        "    * $3^{rd}$ layer is made up by $\\hat{p}_{i,k}$ that depend on the previous layer but also on $p_{i,k}$ with  no dependency;\n",
        "\n",
        "    * Last layer is the loss function which depends on $\\hat{p}_{i,k}$ and $p_{i,k}$;\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fl6vO4IFwTM3",
        "colab_type": "text"
      },
      "source": [
        "### Mecanism to Compute $\\nabla J$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sc73YOgj1MD1"
      },
      "source": [
        "* Gradient of $J$ w.r.t  $\\theta = [w_{11},w_{12},w_{21},w_{22},b_1,b_2]$ is obtained by applying multivariable chaine rule throughout the different layers from up to down:\n",
        "\n",
        "  * Calculate $\\nabla_{\\hat{p}} J$, Gradient of $J$ w.r.t to estimated probabilities $\\hat{p} = [\\hat{p}_{1,1},\\hat{p}_{1,2},\\hat{p}_{2,1},\\hat{p}_{2,2}]$;\n",
        "  \n",
        "  * Calculate $\\left[\\frac{\\partial \\hat{p}}{\\partial s}\\right]$, Jacobian between estimated probabilities $\\hat{p}$ and scores $\\bf{s}$ $ = [s_{1,1},s_{1,2},s_{2,1},s_{2,2}]$;\n",
        "\n",
        "  * Calculate $\\nabla_{s} J$,  Gradient of $J$ w.r.t to scores $s$ with the multivariable chain rule\n",
        "  \\begin{equation}\n",
        "  \\nabla_{s} J = \n",
        "  \\begin{bmatrix}\n",
        "  \\frac{\\partial \\hat{p}}{\\partial s}\n",
        "  \\end{bmatrix}^T\n",
        "  \\nabla_{\\hat{p}} J.\n",
        "  \\end{equation}\n",
        "\n",
        "  * Calculate  $\\left[ \\frac{\\partial \\bf{s}}{\\partial \\theta} \\right]$,  Jacobian between scores $s$ and parameters $\\theta$;\n",
        "  \n",
        "  * Calculate $\\nabla_{\\theta} J$ with the multivariable chaine rule\n",
        "  \n",
        "  \\begin{equation}\n",
        "  \\nabla_{\\theta} J = \n",
        "  \\begin{bmatrix}\n",
        "  \\frac{\\partial s}{\\partial \\theta}\n",
        "  \\end{bmatrix}^T\n",
        "  \\nabla_{s} J.\n",
        "  \\end{equation}  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCwe-MbbXE9P",
        "colab_type": "text"
      },
      "source": [
        "### Notations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v_0vnvitSiOC"
      },
      "source": [
        "* To make implementation easy and simple we organize the computation through the following list of objects:\n",
        "\n",
        "\\\\\n",
        "\n",
        "* $\\bf{X}$: matrix of $n$ examples with $d$ variables;\n",
        "\n",
        "* $\\bf{\\Psi} = [\\bf{X} \\vert 1] \\in  {\\rm I\\!R^{n \\times (d+1)}}$: concatenation of $\\bf{X}$ with column vector of $1$s;\n",
        "\n",
        "* $C$: number of classes;\n",
        "\n",
        "* $\\bf{P} \\in {\\rm I\\!R^{n \\times C}}$: matrix of $n$ examples and their belonging probabilities to $C$ classes;\n",
        "\n",
        "* $\\bf{W} \\in {\\rm I\\!R^{C \\times d}}, \\bf{b} \\in {\\rm I\\!R^{C}}$: respectively weight matrix and bias vector;\n",
        "\n",
        "* $\\bf{\\Theta} = [\\bf{W}\\vert \\bf{b}]$: matrix of all parameters;\n",
        "\n",
        "* $\\Theta_{idx} =  \\{[1,1],\\ldots,[1,d+1],[2,1],\\ldots,[C,d+1]\\}$: set of indices associated to each element of $\\bf{\\Theta}$ (from top to down and left to right). This set of indices is useful to flatten $\\bf{\\Theta}$ in order to manipulate weights and bias as one vector. The first and last elements of $\\bf{\\Theta}$ are accessed as $\\bf{\\Theta}_{\\Theta_{idx}[1]}$ and $\\bf{\\Theta}_{\\Theta_{idx}[C \\times (d+1)]}$;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* $\\bf{S} = \\bf{\\Psi}  \\bf{\\Theta}^T \\in {\\rm I\\!R^{n \\times C}}$: matrix of $C$ scores for the $n$ examples in $\\bf{X}$  \n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\bf{S} = \n",
        "\\begin{bmatrix}\n",
        "\\sum_{f = 1}^{d+1} \\Psi_{1f} \\Theta_{1f} & \\cdots & \\sum_{f = 1}^{d+1} \\Psi_{1f} \\Theta_{Cf}\\\\\n",
        "\\vdots & \\ddots & \\vdots\\\\\n",
        "\\sum_{f = 1}^{d+1} \\Psi_{nf} \\Theta_{1f} & \\cdots & \\sum_{f = 1}^{d+1} \\Psi_{nf} \\Theta_{Cf}\n",
        "\\end{bmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "  * $S_{idx} = \\{[1,1], [1,2],\\ldots,[1,C],[2,1],\\ldots,[2,C],\\ldots, [n,C]\\}$: set of indices of $\\bf{S}$. \n",
        "  \n",
        "* $\\bf{\\hat{P}} = \\mbox{softmax}(\\bf{S}) \\in  {\\rm I\\!R^{n \\times d}}$: matrix of $n$ examples and their estimated belonging probabilities to the $C$ classes.\n",
        "* The flattening of $\\bf{\\hat{P}}$ and $\\bf{P}$ is handled with the set of indices $\\hat{P}_{idx}$ and $P_{idx}$ both equal to $S_{idx}$ as they all refer to matrices of dimensionality $n \\times C$.   \n",
        "\n",
        "* Matrices $\\bf{\\Theta}, \\bf{S}, \\bf{\\hat{P}}, \\bf{P}$ and $J$ are respectively flattened and organized bottom-up  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IIkshF7Q78U7"
      },
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1Ug6TtX072rvEGoQ0HYDmvGmp4SjEnnKe)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il7P30joaPec",
        "colab_type": "text"
      },
      "source": [
        "### Computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iDlBT_g4-Dy_"
      },
      "source": [
        "* Derivations are carried out from top to down;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsQS4jxHATZl",
        "colab_type": "text"
      },
      "source": [
        "#### Gradient of $J$ w.r.t $\\bf{\\hat{P}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwsnlcQWAPGQ",
        "colab_type": "text"
      },
      "source": [
        "* First, we compute gradient of $J$ w.r.t $\\bf{\\hat{P}}$\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_{\\hat{p}} J = \n",
        "\\frac{-1}{n}\n",
        "\\begin{bmatrix}\n",
        "\\frac{P_{P_{idx}[1]}}{\\hat{P}_{P_{idx}[1]}}\\\\\n",
        "\\vdots\\\\\n",
        "\\frac{P_{P_{idx}[n \\times C]}}{\\hat{P}_{P_{idx}[n \\times C]}}\n",
        "\\end{bmatrix}.\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq8_egEuAgSS",
        "colab_type": "text"
      },
      "source": [
        "#### Jacobian between  elements of $\\bf{\\hat{P}}$ and $\\bf{S}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yS4_2WRkAgly",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "* As a start, consider softmax function on one vector of scores $[s_1,\\ldots,s_C]$\n",
        "  \n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\hat{p}_i = \\frac{e^{s_i}}{\\sum_{j = 1}^{C} e^{s_j}}. \n",
        "\\end{equation}\n",
        "\n",
        "* Partial derivative of $\\hat{p}\n",
        "_i$ w.r.t to $s_j$ is\n",
        "\n",
        "\\\\\n",
        "\n",
        "  \\begin{equation}\n",
        "  \\frac{\\partial \\hat{p}_i}{\\partial s_j} =        1_{\\{i = j\\}} \\hat{p}_i - \\hat{p}_i \\hat{p}_j\n",
        "  \\end{equation}\n",
        "  \n",
        " \n",
        " \\\\\n",
        "  \n",
        "* where $1_{\\{i = j\\}}$ an ***indicator function***  equals to $1$ when the condition $i = j$ is satisfied;\n",
        "\n",
        "\\\\\n",
        "\n",
        "\n",
        "* Let $i = [i_1,i_2]\\in \\hat{P}_{idx}$ and  $j = [j_1, j_2]\\in \\hat{S}_{idx}$ be two coordinate of indices respectively referring to entries of $\\bf{\\hat{P}}$ and $\\bf{S}$. The Jacobian $[\\frac{\\partial \\hat{p}}{\\partial s}]$ is the matrix of partial derivatives of any $\\hat{p}_{i_1, i_2}$ w.r.t any $s_{j_1,j_2}$;\n",
        "\n",
        "\\\\\n",
        "\n",
        "* The generic expression can be obtained from the vectorial case with an additional indicator function\n",
        "\n",
        "\\\\\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial \\hat{p}_i}{\\partial s_j} = \n",
        "1_{\\{i_1 = j_1\\}} \\left[  \\left(1_{\\{i_2 = j_2\\}} \\hat{p}_{i}   \\right) - \\left( \\hat{p}_i \\hat{p}_j \\right)  \\right].\n",
        "\\end{equation}\n",
        "\n",
        "\\\\\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vU7UhPRACpUsgvcVHLqeO7UbB_b7TpoB)\n",
        "\n",
        "\n",
        "* Keep in mind that $i$ and $j$ are **coordinates of indices** and  thus $\\hat{p}_i = \\hat{p}_{i_1,i_2}$;\n",
        "   \n",
        "* Indicator function $1_{\\{i_1 = j_1\\}} = 0$ when no computational link exists between $s_j$ and $p_i$;\n",
        "\n",
        "* For example it is obvious that  $\\frac{\\partial \\hat{p}_{1,1}}{\\partial s_{2,1}} = 0$ as the score $s_{2,1}$ appears nowhere in the expression of $\\hat{p}_{1,2}$.\n",
        "\n",
        "\\\\\n",
        "\n",
        "* To populate the Jacobian $\\left[\\frac{\\partial\\hat{p}}{\\partial s} \\right]$:\n",
        "  * iterate over $i \\in \\hat{P}_{idx}$;\n",
        " \n",
        "  * For each given $i$, iterate over all $j \\in \\hat{S}_{idx}$ and compute $\\frac{\\partial \\hat{p}_i}{\\partial s_j}$.\n",
        "     \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeuezSNED4IH",
        "colab_type": "text"
      },
      "source": [
        "#### Gradient of $J$ w.r.t $s$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQJri2l_D4Xb",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "\\begin{equation}\n",
        "\\nabla_{s} J = \n",
        "\\begin{bmatrix}\n",
        "  \\frac{\\partial \\hat{p}}{\\partial s} \n",
        "  \\end{bmatrix}^T \\nabla_{\\hat{p}} J.\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjTZHd5XEDn-",
        "colab_type": "text"
      },
      "source": [
        "#### Jacobian between  elements of $\\bf{S}$ and $\\bf{\\Theta}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWkJBWGqEDze",
        "colab_type": "text"
      },
      "source": [
        " * Let $i \\in S_{idx}$ and $j \\in \\Theta_{idx}$ be two coordinates of indices, the partial derivative of $s_i$ w.r.t $\\theta_j$ is\n",
        " \n",
        " \\begin{equation}\n",
        " \\frac{\\partial s_i}{\\partial \\theta_j} = 1_{ \\{  \\{i_2 = j_1\\}  or  \\{j_1 = d+1\\}  \\}} \\Psi_{i_1,j_1}, \n",
        " \\end{equation}\n",
        " \n",
        " * the indicator function  $1_{ \\{  \\{i_2 = j_1\\}  or  \\{j_1 = d+1\\}  \\}}$ contains $2$ conditions linked with a logical ***or*** which means that the function equals to one if ***at least*** one condition holds. The result should be easy to get if you refer to the explicit expression of $\\bf{S}$ above.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwP-1safEfnG",
        "colab_type": "text"
      },
      "source": [
        "####  Gradient of $J$ w.r.t $\\theta$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg0gRpiLEfyK",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\\begin{equation}\n",
        "\\nabla_{\\theta} J = \n",
        "\\begin{bmatrix}\n",
        "\\frac{\\partial s}{\\partial \\theta}\n",
        "\\end{bmatrix}^T\n",
        "\\nabla_{s} J.\n",
        "\\end{equation}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xQ2Wr2-ry1MD"
      },
      "source": [
        "### Exercice in Python: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAcD1gz6c69Y",
        "colab_type": "text"
      },
      "source": [
        "#### Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yS8myZbXzfyV"
      },
      "source": [
        "\n",
        "* Implement gradient descent to estimate parameters of softmax model with $2$ classes using only *numpy*.\n",
        "\n",
        "* Dataset can be downloaded, splitted, normalized and visualized through the code below;\n",
        "\n",
        "* Guidlines will be given for each function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc66lkw3Z6jn",
        "colab_type": "code",
        "outputId": "f5aa74c9-865e-4c1f-db74-89a464aa7899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import requests \n",
        "from io import BytesIO\n",
        "import pandas as pd\n",
        "\n",
        "def download_csv(spread_sheet_id):\n",
        "    try:\n",
        "        r = requests.get('https://docs.google.com/spreadsheet/ccc?key='+ spread_sheet_id + '&output=csv')\n",
        "        req_content = r.content\n",
        "        df = pd.read_csv(BytesIO(req_content))\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "dataset_id = '1OLZyOG14rjuCcpM_qnCPVWBsihqYQpu8vw7LNEoaBWQ'\n",
        "\n",
        "dataset_df = download_csv(dataset_id) \n",
        "display(dataset_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature_1</th>\n",
              "      <th>feature_2</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>76.284982</td>\n",
              "      <td>60.928198</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>76.020258</td>\n",
              "      <td>60.797732</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>76.001697</td>\n",
              "      <td>62.508987</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>80.181739</td>\n",
              "      <td>64.425732</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78.467312</td>\n",
              "      <td>61.504805</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>78.252342</td>\n",
              "      <td>61.528269</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>77.881669</td>\n",
              "      <td>57.537015</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>76.056499</td>\n",
              "      <td>64.518068</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>80.097033</td>\n",
              "      <td>59.205459</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>76.462411</td>\n",
              "      <td>58.254293</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>76.532905</td>\n",
              "      <td>63.391683</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>81.813037</td>\n",
              "      <td>57.794224</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>72.699649</td>\n",
              "      <td>58.061095</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>77.213404</td>\n",
              "      <td>57.757270</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>76.679257</td>\n",
              "      <td>63.461161</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>74.739075</td>\n",
              "      <td>57.442868</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>79.020948</td>\n",
              "      <td>61.641011</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>79.010728</td>\n",
              "      <td>57.413238</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>81.255086</td>\n",
              "      <td>61.453509</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>78.691649</td>\n",
              "      <td>57.448168</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>76.644596</td>\n",
              "      <td>60.861159</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>76.986072</td>\n",
              "      <td>61.890529</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>77.665627</td>\n",
              "      <td>60.862211</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>76.459880</td>\n",
              "      <td>62.497705</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>74.276068</td>\n",
              "      <td>61.447592</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>76.281982</td>\n",
              "      <td>57.866243</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>78.359785</td>\n",
              "      <td>57.494943</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>81.442303</td>\n",
              "      <td>62.281087</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>76.353746</td>\n",
              "      <td>56.707815</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>75.659582</td>\n",
              "      <td>57.514569</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>79.380286</td>\n",
              "      <td>66.146315</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>81.506206</td>\n",
              "      <td>70.234188</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>78.288736</td>\n",
              "      <td>66.904320</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>78.160645</td>\n",
              "      <td>70.632142</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>80.764158</td>\n",
              "      <td>69.862973</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>80.775833</td>\n",
              "      <td>67.399549</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>80.993590</td>\n",
              "      <td>69.982021</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>78.275087</td>\n",
              "      <td>69.176533</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>178</th>\n",
              "      <td>83.492507</td>\n",
              "      <td>72.616037</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>179</th>\n",
              "      <td>80.098318</td>\n",
              "      <td>69.921021</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>180</th>\n",
              "      <td>80.469110</td>\n",
              "      <td>66.496748</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>84.890842</td>\n",
              "      <td>68.629883</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182</th>\n",
              "      <td>78.738007</td>\n",
              "      <td>75.471301</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>183</th>\n",
              "      <td>85.541291</td>\n",
              "      <td>70.172513</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>84.618302</td>\n",
              "      <td>70.288251</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185</th>\n",
              "      <td>84.977963</td>\n",
              "      <td>71.807054</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>186</th>\n",
              "      <td>79.654574</td>\n",
              "      <td>67.807219</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187</th>\n",
              "      <td>82.384679</td>\n",
              "      <td>68.660923</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>188</th>\n",
              "      <td>80.958147</td>\n",
              "      <td>72.908409</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>189</th>\n",
              "      <td>79.814692</td>\n",
              "      <td>70.066581</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>190</th>\n",
              "      <td>81.405856</td>\n",
              "      <td>67.773252</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>191</th>\n",
              "      <td>75.102120</td>\n",
              "      <td>70.904058</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>192</th>\n",
              "      <td>77.225946</td>\n",
              "      <td>66.192204</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>193</th>\n",
              "      <td>82.028764</td>\n",
              "      <td>70.166100</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>80.954167</td>\n",
              "      <td>65.899685</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>79.558950</td>\n",
              "      <td>72.429533</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>80.531614</td>\n",
              "      <td>67.864212</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>78.230803</td>\n",
              "      <td>72.025471</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>77.380942</td>\n",
              "      <td>68.576689</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>80.249036</td>\n",
              "      <td>66.005061</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows  3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     feature_1  feature_2  class\n",
              "0    76.284982  60.928198      0\n",
              "1    76.020258  60.797732      0\n",
              "2    76.001697  62.508987      0\n",
              "3    80.181739  64.425732      0\n",
              "4    78.467312  61.504805      0\n",
              "5    78.252342  61.528269      0\n",
              "6    77.881669  57.537015      0\n",
              "7    76.056499  64.518068      0\n",
              "8    80.097033  59.205459      0\n",
              "9    76.462411  58.254293      0\n",
              "10   76.532905  63.391683      0\n",
              "11   81.813037  57.794224      0\n",
              "12   72.699649  58.061095      0\n",
              "13   77.213404  57.757270      0\n",
              "14   76.679257  63.461161      0\n",
              "15   74.739075  57.442868      0\n",
              "16   79.020948  61.641011      0\n",
              "17   79.010728  57.413238      0\n",
              "18   81.255086  61.453509      0\n",
              "19   78.691649  57.448168      0\n",
              "20   76.644596  60.861159      0\n",
              "21   76.986072  61.890529      0\n",
              "22   77.665627  60.862211      0\n",
              "23   76.459880  62.497705      0\n",
              "24   74.276068  61.447592      0\n",
              "25   76.281982  57.866243      0\n",
              "26   78.359785  57.494943      0\n",
              "27   81.442303  62.281087      0\n",
              "28   76.353746  56.707815      0\n",
              "29   75.659582  57.514569      0\n",
              "..         ...        ...    ...\n",
              "170  79.380286  66.146315      1\n",
              "171  81.506206  70.234188      1\n",
              "172  78.288736  66.904320      1\n",
              "173  78.160645  70.632142      1\n",
              "174  80.764158  69.862973      1\n",
              "175  80.775833  67.399549      1\n",
              "176  80.993590  69.982021      1\n",
              "177  78.275087  69.176533      1\n",
              "178  83.492507  72.616037      1\n",
              "179  80.098318  69.921021      1\n",
              "180  80.469110  66.496748      1\n",
              "181  84.890842  68.629883      1\n",
              "182  78.738007  75.471301      1\n",
              "183  85.541291  70.172513      1\n",
              "184  84.618302  70.288251      1\n",
              "185  84.977963  71.807054      1\n",
              "186  79.654574  67.807219      1\n",
              "187  82.384679  68.660923      1\n",
              "188  80.958147  72.908409      1\n",
              "189  79.814692  70.066581      1\n",
              "190  81.405856  67.773252      1\n",
              "191  75.102120  70.904058      1\n",
              "192  77.225946  66.192204      1\n",
              "193  82.028764  70.166100      1\n",
              "194  80.954167  65.899685      1\n",
              "195  79.558950  72.429533      1\n",
              "196  80.531614  67.864212      1\n",
              "197  78.230803  72.025471      1\n",
              "198  77.380942  68.576689      1\n",
              "199  80.249036  66.005061      1\n",
              "\n",
              "[200 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WByKeh-dAVul",
        "outputId": "de8b25fc-afad-4df4-9efd-c9221ef58386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "X = dataset_df.iloc[:,0:2].to_numpy()\n",
        "y = dataset_df.iloc[:,2].to_numpy()\n",
        "P = np.zeros(X.shape)\n",
        "P[np.arange(0,len(X)),y] = 1\n",
        "W,b = np.zeros([P.shape[1],X.shape[1]]), np.zeros([P.shape[1],1])\n",
        "\n",
        "\n",
        "def plot_softmax_model(X,y,W,b,lines):\n",
        "\n",
        "    class_colors = ['blue', 'red', 'green', 'purple', 'pink']\n",
        "          \n",
        "    if lines:\n",
        "        X_std = (X - np.mean(X, axis = 0))/np.std(X, axis = 0)\n",
        "    else:\n",
        "        X_std = X\n",
        "\n",
        "    line_X_stds = np.arange(min(X_std[:,0])-2, max(X_std[:,0]) + 2) \n",
        "   \n",
        "    plt.figure(figsize = (6,6))\n",
        "    plt.rcParams['axes.facecolor'] = 'lightsteelblue'\n",
        "    plt.grid(c='white')\n",
        "\n",
        "    plt.xlim(min(X_std[:,0])-2, max(X_std[:,0]) + 2)\n",
        "    plt.ylim(min(X_std[:,1])-2, max(X_std[:,1]) + 2)\n",
        "      \n",
        "    for i in range(W.shape[0]):       \n",
        "        plt.scatter(X_std[np.where(y == i),0], X_std[np.where(y == i),1], marker = 'o', color = class_colors[i])\n",
        "\n",
        "    if lines:   \n",
        "        for i in range(W.shape[0]):\n",
        "            plt.plot(line_X_stds, -1*(W[i,0]/W[i,1])*line_X_stds - (b[i]/W[i,1]), color = class_colors[i])\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_softmax_model(X,y,W,b,False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAFpCAYAAACWFzOYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nG1sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eH1x8sHf32yD"
      },
      "source": [
        "#### Softmax Model\n",
        "\n",
        "* The softmax function is implemented according to the following expression \n",
        "\\begin{equation}\n",
        "\\hat{p}_i = \\frac{e^{s_i - max(s)}}{\\sum_{j = 1}^C e^{e_j - max(s)}} = \\frac{e^{s_i}}{\\sum_{j = 1}^C e^{e_j }}. \n",
        "\\end{equation}\n",
        "\n",
        "*  Indeed $s_j \\in{\\rm I\\!R}$ and thus have no limits. The value of $e^{s_j}$ may easily become so large that it will  no longer be representable in the computer resulting in an ***overfloating*** error. To prevent this, we substract the biggest score from every $s_j$. Simple math will show that two  expressions are exactly the same.   "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DDA0wVGW01dl",
        "colab": {}
      },
      "source": [
        "def softmax(X,W,b):\n",
        "    S = np.transpose(W.dot(X.transpose()) + b)\n",
        "    S_max = np.max(S, axis = 1)\n",
        "    S_minus_S_max = S - S_max.reshape([len(S_max), 1])\n",
        "    P_hat = np.exp(S_minus_S_max)/np.sum(np.exp(S_minus_S_max), axis = 1).reshape([len(X),1])\n",
        "    return P_hat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "miZJDqkt9LKZ"
      },
      "source": [
        "#### Error Functions\n",
        "\n",
        "* we implement the negative log-likelihood function but also the error of classification which is simply the rate of rightly classified examples in $\\bf{X}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SjMy2hU58bHC",
        "colab": {}
      },
      "source": [
        "def neg_log_likelihood(X,P,W,b):\n",
        "    return (-1/(len(X))) * np.sum(np.sum((P * np.log(softmax(X,W,b))), axis = 1))\n",
        "\n",
        "def error_classification(X,y,W,b): \n",
        "    return np.sum(y == np.argmax(softmax(X,W,b), axis = 1))/len(y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GK-Ow91M9jNG"
      },
      "source": [
        "#### Gradient of $J$ w.r.t parameters\n",
        "\n",
        " * we implement the expressions of the gradients and jacobians according to the formalism explicited above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_KlzcLXp9ccH",
        "colab": {}
      },
      "source": [
        "def gradient_neg_log_likelihood(X,y,P,W,b):\n",
        "   \n",
        "    X_1 = np.hstack([X, np.ones([len(X),1])])\n",
        "    theta = np.hstack([W,b])\n",
        "    \n",
        "    theta_idx = list(np.ndindex(theta.shape))\n",
        "    P_idx = S_idx = list(np.ndindex((X.shape[0], len(b))))\n",
        "    \n",
        "    jacobian_s = np.array([X_1[i[0], j[1]] * ((i[1] == j[0]) or (j[0] == (X.shape[1]))) \n",
        "                  for (i,j) in [(i,j) for i in S_idx for j in theta_idx]]).reshape(X.shape[0]*len(b), np.product(theta.shape))  \n",
        "    \n",
        "    P_hat = softmax(X,W,b) \n",
        "        \n",
        "    jacobian_idx = [(i,j) for i in P_idx for j in S_idx]\n",
        "    jacobian_p_hat =  np.array([ (i[0] == j[0]) * ( ( (i[1] == j[1]) * P_hat[i]) - ( P_hat[i] * P_hat[j])) \n",
        "                        for (i,j) in jacobian_idx]).reshape([np.prod(P_hat.shape), np.prod(P_hat.shape)])\n",
        "    \n",
        "    gradient_J_p_hat = (-1/len(X)) *( np.array([(P[i]/P_hat[i]) for i in P_idx])).reshape([np.prod(P_hat.shape),1])\n",
        "    \n",
        "    gradient_J_theta = jacobian_s.transpose().dot(jacobian_p_hat.transpose().dot(gradient_J_p_hat))\n",
        "    \n",
        "    partial_theta = gradient_J_theta.reshape(theta.shape)\n",
        "    partial_W, partial_b = partial_theta[:,np.arange(0,W.shape[1])], partial_theta[:,(W.shape[1])].reshape(b.shape)\n",
        "    return partial_W, partial_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Qlhte5F-hc1"
      },
      "source": [
        "#### Gradient Descent Algorithm\n",
        "\n",
        "* Gradient Descent algorithm: \n",
        "  * delta: learning step;\n",
        "  * threshold: the pourcentage of ***classification error*** we want to attain. If this pourcentage cannot be attained the algorithm will stop after $10^6$ iterations;\n",
        "  * random: boolean if True weights are initialized randomly otherwise they are initialized at zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zdByoQ0i9oiO",
        "colab": {}
      },
      "source": [
        "def gradient_descent_softmax(X,y,P,delta,threshold, random = True):\n",
        "   \n",
        "    if random:\n",
        "        W,b = np.random.normal(0,1, size = [P.shape[1],X.shape[1]]), np.zeros([P.shape[1],1]) #np.random.normal(0,1,[P.shape[1],1])#\n",
        "    else:\n",
        "        W,b = np.zeros([P.shape[1],X.shape[1]]), np.zeros([P.shape[1],1])\n",
        "    \n",
        "    i = 0\n",
        "    while (error_classification(X,y,W,b) < threshold) and (i < 1e6):\n",
        "        partial_W, partial_b = gradient_neg_log_likelihood(X,y,P,W,b)\n",
        "        W, b = W - (delta*partial_W), b - (delta*partial_b)\n",
        "        print('Iteration %i CrossEntropy = %f and Precision = %f%s' % (i, neg_log_likelihood(X,P,W,b), error_classification(X,y,W,b)*100,'%'))\n",
        "        i = i + 1\n",
        "\n",
        "    print('While Loop Left at Iteration %d with CrossEntropy = %f and Precision = %f'% (i, neg_log_likelihood(X,P,W,b), error_classification(X,y,W,b)*100))\n",
        "    return W,b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UGV0JATEEMP6"
      },
      "source": [
        "#### Execution of Everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oHtJQO_sGgQp",
        "outputId": "cdc3fc87-fcdd-43c9-beb4-36d7e95ddcfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "X = (X - np.mean(X, axis = 0))/np.std(X, axis = 0)\n",
        "W_hat,b_hat = gradient_descent_softmax(X,y,P,0.1, 0.99, random = True)\n",
        "plot_softmax_model(X,y,W_hat,b_hat,True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 0 CrossEntropy = 1.499864 and Precision = 48.000000%\n",
            "Iteration 1 CrossEntropy = 1.408507 and Precision = 49.000000%\n",
            "Iteration 2 CrossEntropy = 1.321763 and Precision = 51.500000%\n",
            "Iteration 3 CrossEntropy = 1.239593 and Precision = 53.500000%\n",
            "Iteration 4 CrossEntropy = 1.161943 and Precision = 54.500000%\n",
            "Iteration 5 CrossEntropy = 1.088745 and Precision = 55.500000%\n",
            "Iteration 6 CrossEntropy = 1.019915 and Precision = 58.500000%\n",
            "Iteration 7 CrossEntropy = 0.955357 and Precision = 59.500000%\n",
            "Iteration 8 CrossEntropy = 0.894958 and Precision = 61.000000%\n",
            "Iteration 9 CrossEntropy = 0.838592 and Precision = 61.500000%\n",
            "Iteration 10 CrossEntropy = 0.786117 and Precision = 63.000000%\n",
            "Iteration 11 CrossEntropy = 0.737379 and Precision = 66.000000%\n",
            "Iteration 12 CrossEntropy = 0.692215 and Precision = 68.000000%\n",
            "Iteration 13 CrossEntropy = 0.650448 and Precision = 69.500000%\n",
            "Iteration 14 CrossEntropy = 0.611894 and Precision = 72.000000%\n",
            "Iteration 15 CrossEntropy = 0.576366 and Precision = 72.500000%\n",
            "Iteration 16 CrossEntropy = 0.543669 and Precision = 74.000000%\n",
            "Iteration 17 CrossEntropy = 0.513611 and Precision = 74.500000%\n",
            "Iteration 18 CrossEntropy = 0.486000 and Precision = 75.500000%\n",
            "Iteration 19 CrossEntropy = 0.460647 and Precision = 77.500000%\n",
            "Iteration 20 CrossEntropy = 0.437370 and Precision = 79.000000%\n",
            "Iteration 21 CrossEntropy = 0.415996 and Precision = 81.000000%\n",
            "Iteration 22 CrossEntropy = 0.396361 and Precision = 83.000000%\n",
            "Iteration 23 CrossEntropy = 0.378311 and Precision = 83.500000%\n",
            "Iteration 24 CrossEntropy = 0.361704 and Precision = 85.000000%\n",
            "Iteration 25 CrossEntropy = 0.346409 and Precision = 86.500000%\n",
            "Iteration 26 CrossEntropy = 0.332307 and Precision = 87.000000%\n",
            "Iteration 27 CrossEntropy = 0.319290 and Precision = 87.500000%\n",
            "Iteration 28 CrossEntropy = 0.307257 and Precision = 87.500000%\n",
            "Iteration 29 CrossEntropy = 0.296121 and Precision = 89.000000%\n",
            "Iteration 30 CrossEntropy = 0.285799 and Precision = 89.500000%\n",
            "Iteration 31 CrossEntropy = 0.276220 and Precision = 90.000000%\n",
            "Iteration 32 CrossEntropy = 0.267317 and Precision = 90.000000%\n",
            "Iteration 33 CrossEntropy = 0.259030 and Precision = 91.000000%\n",
            "Iteration 34 CrossEntropy = 0.251306 and Precision = 91.500000%\n",
            "Iteration 35 CrossEntropy = 0.244097 and Precision = 92.000000%\n",
            "Iteration 36 CrossEntropy = 0.237359 and Precision = 92.000000%\n",
            "Iteration 37 CrossEntropy = 0.231051 and Precision = 92.000000%\n",
            "Iteration 38 CrossEntropy = 0.225138 and Precision = 92.000000%\n",
            "Iteration 39 CrossEntropy = 0.219589 and Precision = 92.500000%\n",
            "Iteration 40 CrossEntropy = 0.214372 and Precision = 93.500000%\n",
            "Iteration 41 CrossEntropy = 0.209462 and Precision = 93.500000%\n",
            "Iteration 42 CrossEntropy = 0.204835 and Precision = 93.500000%\n",
            "Iteration 43 CrossEntropy = 0.200468 and Precision = 93.500000%\n",
            "Iteration 44 CrossEntropy = 0.196342 and Precision = 94.500000%\n",
            "Iteration 45 CrossEntropy = 0.192438 and Precision = 95.000000%\n",
            "Iteration 46 CrossEntropy = 0.188741 and Precision = 95.000000%\n",
            "Iteration 47 CrossEntropy = 0.185234 and Precision = 95.000000%\n",
            "Iteration 48 CrossEntropy = 0.181906 and Precision = 95.000000%\n",
            "Iteration 49 CrossEntropy = 0.178742 and Precision = 95.000000%\n",
            "Iteration 50 CrossEntropy = 0.175732 and Precision = 95.000000%\n",
            "Iteration 51 CrossEntropy = 0.172865 and Precision = 95.000000%\n",
            "Iteration 52 CrossEntropy = 0.170132 and Precision = 96.000000%\n",
            "Iteration 53 CrossEntropy = 0.167524 and Precision = 96.000000%\n",
            "Iteration 54 CrossEntropy = 0.165033 and Precision = 96.000000%\n",
            "Iteration 55 CrossEntropy = 0.162652 and Precision = 96.000000%\n",
            "Iteration 56 CrossEntropy = 0.160373 and Precision = 96.000000%\n",
            "Iteration 57 CrossEntropy = 0.158191 and Precision = 96.500000%\n",
            "Iteration 58 CrossEntropy = 0.156100 and Precision = 96.500000%\n",
            "Iteration 59 CrossEntropy = 0.154094 and Precision = 96.500000%\n",
            "Iteration 60 CrossEntropy = 0.152169 and Precision = 96.500000%\n",
            "Iteration 61 CrossEntropy = 0.150320 and Precision = 96.500000%\n",
            "Iteration 62 CrossEntropy = 0.148543 and Precision = 96.500000%\n",
            "Iteration 63 CrossEntropy = 0.146833 and Precision = 96.500000%\n",
            "Iteration 64 CrossEntropy = 0.145188 and Precision = 96.500000%\n",
            "Iteration 65 CrossEntropy = 0.143603 and Precision = 97.000000%\n",
            "Iteration 66 CrossEntropy = 0.142076 and Precision = 97.000000%\n",
            "Iteration 67 CrossEntropy = 0.140603 and Precision = 97.000000%\n",
            "Iteration 68 CrossEntropy = 0.139182 and Precision = 97.000000%\n",
            "Iteration 69 CrossEntropy = 0.137811 and Precision = 97.000000%\n",
            "Iteration 70 CrossEntropy = 0.136486 and Precision = 97.000000%\n",
            "Iteration 71 CrossEntropy = 0.135206 and Precision = 97.000000%\n",
            "Iteration 72 CrossEntropy = 0.133968 and Precision = 97.500000%\n",
            "Iteration 73 CrossEntropy = 0.132771 and Precision = 97.500000%\n",
            "Iteration 74 CrossEntropy = 0.131612 and Precision = 97.500000%\n",
            "Iteration 75 CrossEntropy = 0.130489 and Precision = 97.500000%\n",
            "Iteration 76 CrossEntropy = 0.129402 and Precision = 97.500000%\n",
            "Iteration 77 CrossEntropy = 0.128349 and Precision = 98.000000%\n",
            "Iteration 78 CrossEntropy = 0.127327 and Precision = 98.000000%\n",
            "Iteration 79 CrossEntropy = 0.126336 and Precision = 98.000000%\n",
            "Iteration 80 CrossEntropy = 0.125374 and Precision = 98.500000%\n",
            "Iteration 81 CrossEntropy = 0.124441 and Precision = 98.500000%\n",
            "Iteration 82 CrossEntropy = 0.123534 and Precision = 98.500000%\n",
            "Iteration 83 CrossEntropy = 0.122653 and Precision = 98.500000%\n",
            "Iteration 84 CrossEntropy = 0.121796 and Precision = 98.500000%\n",
            "Iteration 85 CrossEntropy = 0.120964 and Precision = 98.500000%\n",
            "Iteration 86 CrossEntropy = 0.120153 and Precision = 98.500000%\n",
            "Iteration 87 CrossEntropy = 0.119365 and Precision = 98.500000%\n",
            "Iteration 88 CrossEntropy = 0.118598 and Precision = 98.500000%\n",
            "Iteration 89 CrossEntropy = 0.117851 and Precision = 98.500000%\n",
            "Iteration 90 CrossEntropy = 0.117123 and Precision = 98.000000%\n",
            "Iteration 91 CrossEntropy = 0.116414 and Precision = 98.000000%\n",
            "Iteration 92 CrossEntropy = 0.115723 and Precision = 98.000000%\n",
            "Iteration 93 CrossEntropy = 0.115049 and Precision = 98.000000%\n",
            "Iteration 94 CrossEntropy = 0.114391 and Precision = 98.000000%\n",
            "Iteration 95 CrossEntropy = 0.113750 and Precision = 98.500000%\n",
            "Iteration 96 CrossEntropy = 0.113124 and Precision = 98.500000%\n",
            "Iteration 97 CrossEntropy = 0.112513 and Precision = 98.500000%\n",
            "Iteration 98 CrossEntropy = 0.111917 and Precision = 99.000000%\n",
            "While Loop Left at Iteration 99 with CrossEntropy = 0.111917 and Precision = 99.000000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAFpCAYAAACI3gMrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}